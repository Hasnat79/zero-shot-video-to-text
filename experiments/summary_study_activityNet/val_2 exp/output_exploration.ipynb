{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/zero-shot-video-to-text/experiments/summary_study_activityNet/val_2 exp/(0-55)output_val_2_summary_id_caption.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a8b321636294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Load data from three JSON files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdict1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/zero-shot-video-to-text/experiments/summary_study_activityNet/val_2 exp/(0-55)output_val_2_summary_id_caption.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mdict2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/zero-shot-video-to-text/experiments/summary_study_activityNet/val_2 exp/(55-110)output_val_2_summary_id_caption.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdict3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/zero-shot-video-to-text/experiments/summary_study_activityNet/val_2 exp/output_val_2_summary_id_caption.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a8b321636294>\u001b[0m in \u001b[0;36mload_json_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/zero-shot-video-to-text/experiments/summary_study_activityNet/val_2 exp/(0-55)output_val_2_summary_id_caption.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def merge_dicts(dict1, dict2, dict3):\n",
    "    merged_dict = {**dict1, **dict2, **dict3}\n",
    "    return merged_dict\n",
    "\n",
    "def write_to_json_file(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data from three JSON files\n",
    "    dict1 = load_json_file(\"/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/zero-shot-video-to-text/experiments/summary_study_activityNet/val_2 exp/(0-55)output_val_2_summary_id_caption.json\")\n",
    "    dict2 = load_json_file(\"/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/zero-shot-video-to-text/experiments/summary_study_activityNet/val_2 exp/(55-110)output_val_2_summary_id_caption.json\")\n",
    "    dict3 = load_json_file(\"/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/zero-shot-video-to-text/experiments/summary_study_activityNet/val_2 exp/output_val_2_summary_id_caption.json\")\n",
    "\n",
    "    # Merge the dictionaries\n",
    "    merged_dict = merge_dicts(dict1, dict2, dict3)\n",
    "\n",
    "    # Write the merged dictionary to a new JSON file\n",
    "    write_to_json_file(merged_dict, \"/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/zero-shot-video-to-text/experiments/summary_study_activityNet/val_2 exp/output_val_2_summary_id_caption.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_val_2_summary_id_caption = load_json_file(\"/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/zero-shot-video-to-text/experiments/summary_study_activityNet/val_2 exp/output_val_2_summary_id_caption.json\")\n",
    "len(output_val_2_summary_id_caption)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation of val 2 set on zero-shot captioner. \n",
    "### bleu\n",
    "BLEU@1: 0.0614\n",
    "BLEU@2: 0.0000\n",
    "BLEU@3: 0.0000\n",
    "BLEU@4: 0.0000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434\n",
      "len(reference_captions) 331\n",
      "len(candidate_captions) 331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/oeaa/lib/python3.6/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/oeaa/lib/python3.6/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/oeaa/lib/python3.6/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative BLEU scores:\n",
      "BLEU@1: 0.0615\n",
      "BLEU@2: 0.0000\n",
      "BLEU@3: 0.0000\n",
      "BLEU@4: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "evaluating the generated summaries of 50 (least freq labels) videos\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "import statistics\n",
    "\n",
    "import string\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def remove_punctuation(sentence):\n",
    "    # Remove punctuations from the sentence\n",
    "    no_punct = sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Lowercase all the words\n",
    "    lowercase_sentence = no_punct.lower()\n",
    "\n",
    "    # stem words\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    sentence = \" \".join([stemmer.stem(word) for word in lowercase_sentence.split()])\n",
    "    return sentence\n",
    "\n",
    "# # Example usage\n",
    "# sentence = \"Hello, World!\"\n",
    "# clean_sentence = remove_punctuation(sentence)\n",
    "# print(clean_sentence)\n",
    "\n",
    "output_val_2_summary_id_caption = load_json_file(\"/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/zero-shot-video-to-text/experiments/summary_study_activityNet/val_2 exp/output_val_2_summary_id_caption.json\")\n",
    "\n",
    "\n",
    "# bleu score calculation\n",
    "reference_captions = []\n",
    "candidate_captions = []\n",
    "print(len(output_val_2_summary_id_caption))\n",
    "for k,v in output_val_2_summary_id_caption.items():\n",
    "    # print(k)\n",
    "    if v['generated_summary']!=\"_\":\n",
    "        # print(remove_punctuation(v['summary_cap']))\n",
    "        # print(remove_punctuation(v['generated_summary']))\n",
    "\n",
    "        reference_captions.append(remove_punctuation(v['summary_cap']))\n",
    "        candidate_captions.append(remove_punctuation(v['generated_summary']))\n",
    "\n",
    "\n",
    "# Tokenize the captions\n",
    "reference_captions = [caption.split() for caption in reference_captions]\n",
    "candidate_captions = [caption.split() for caption in candidate_captions]\n",
    "\n",
    "print(\"len(reference_captions)\",len(reference_captions))\n",
    "print(\"len(candidate_captions)\",len(candidate_captions))\n",
    "# Calculate BLEU score\n",
    "cumulative_bleu_scores = []\n",
    "weight_list = [(1, 0, 0, 0),(0.5, 0.5, 0, 0),(0.33, 0.33, 0.33, 0),(0.25, 0.25, 0.25, 0.25)]\n",
    "for n in range(1, 5):\n",
    "    bleu_scores = [sentence_bleu(ref, cand, weights=weight_list[n-1]) for ref, cand in zip(reference_captions, candidate_captions)]\n",
    "    \n",
    "    # print(type(bleu_scores))\n",
    "\n",
    "    average_bleu_score = statistics.mean(bleu_scores)\n",
    "    cumulative_bleu_scores.append(average_bleu_score)\n",
    "\n",
    "# Print cumulative BLEU scores\n",
    "print(\"cumulative BLEU scores:\")\n",
    "for i, score in enumerate(cumulative_bleu_scores):\n",
    "    print(f\"BLEU@{i+1}: {score:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate meteor score of val 2\n",
    "METEOR Score: 0.09812680535507115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434\n"
     ]
    }
   ],
   "source": [
    "output_val_2_summary_id_caption = load_json_file(\"/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/zero-shot-video-to-text/experiments/summary_study_activityNet/val_2 exp/output_val_2_summary_id_caption.json\")\n",
    "def remove_punctuation(sentence):\n",
    "    # Remove punctuations from the sentence\n",
    "    no_punct = sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Lowercase all the words\n",
    "    lowercase_sentence = no_punct.lower()\n",
    "\n",
    "    # stem words\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    sentence = \" \".join([stemmer.stem(word) for word in lowercase_sentence.split()])\n",
    "    return sentence\n",
    "\n",
    "reference_captions = []\n",
    "candidate_captions = []\n",
    "print(len(output_val_2_summary_id_caption))\n",
    "for k,v in output_val_2_summary_id_caption.items():\n",
    "    if v['generated_summary']!=\"_\":\n",
    "        reference_captions.append(remove_punctuation(v['summary_cap']))\n",
    "        candidate_captions.append(remove_punctuation(v['generated_summary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: 0.1145287904126113\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def calculate_meteor_score(ref_captions, cand_captions):\n",
    "    scores = []\n",
    "    for ref, cand in zip(ref_captions, cand_captions):\n",
    "        score = meteor_score([ref], cand)\n",
    "        scores.append(score)\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    return average_score\n",
    "\n",
    "# # Tokenize the captions\n",
    "reference_captions = [caption.split() for caption in reference_captions]\n",
    "candidate_captions = [caption.split() for caption in candidate_captions]\n",
    "\n",
    "meteor_score = calculate_meteor_score(reference_captions, candidate_captions)\n",
    "print(\"METEOR Score:\", meteor_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/grads/h/hasnat.md.abdullah/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2e1f1f1484f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_val_2_summary_id_caption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generated_summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mreference_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreference_captions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mcandidate_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidate_captions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mreference_captions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_captions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2e1f1f1484f2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_val_2_summary_id_caption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generated_summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mreference_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreference_captions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mcandidate_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidate_captions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mreference_captions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_captions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2e1f1f1484f2>\u001b[0m in \u001b[0;36mremove_punctuation\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Remove punctuations from the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mno_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Lowercase all the words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mlowercase_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_punct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'translate'"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import json\n",
    "# from pycocoevalcap.cider import Cider\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('wordnet')\n",
    "# cider \n",
    "def cal_cider_score(reference_tokens, candidate_tokens):\n",
    "\n",
    "    # print('reference_tokens', reference_tokens)\n",
    "    # print('candidate_tokens', candidate_tokens)\n",
    "\n",
    "    # Convert tokens to string for TF-IDF vectorization\n",
    "    generated_text = ' '.join(candidate_tokens)\n",
    "    reference_texts = [' '.join(reference_tokens)]\n",
    "\n",
    "    # print('generated_text', generated_text)\n",
    "    # print('reference_texts', reference_texts)\n",
    "\n",
    "    # TF-IDF vectorization\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([generated_text] + reference_texts)\n",
    "\n",
    "    # print('type(tfidf_matrix)', type(tfidf_matrix))\n",
    "    # print('tfidf_matrix:', tfidf_matrix)\n",
    "    # print('tfidf_matrix[0]:', tfidf_matrix[0])\n",
    "    # print('tfidf_matrix[1:]:', tfidf_matrix[1:])\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1:])\n",
    "    # print('similarities', similarities)\n",
    "\n",
    "    # Compute consensus score\n",
    "    consensus_score = similarities.mean()\n",
    "    # print('consensus_score', consensus_score)\n",
    "\n",
    "    # Compute sentence length penalty\n",
    "    generated_length = len(candidate_tokens)\n",
    "    # reference_lengths = [len(ref) for ref in reference_tokens] # this applies when there are multiple references\n",
    "    reference_lengths = len(reference_tokens) # this applies when there is only one reference\n",
    "    # print('generated_length', generated_length)\n",
    "    # print('reference_lengths', reference_lengths)\n",
    "\n",
    "    length_penalty = max(0, 1 - abs(generated_length - np.mean(reference_lengths)) / np.mean(reference_lengths))\n",
    "    # print('length_penalty', length_penalty)\n",
    "\n",
    "    # Compute CIDEr score\n",
    "    cider_score = consensus_score * length_penalty\n",
    "    # print('cider_score', cider_score)\n",
    "    # stop\n",
    "\n",
    "    return cider_score\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def remove_punctuation(sentence):\n",
    "    # Remove punctuations from the sentence\n",
    "    no_punct = sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Lowercase all the words\n",
    "    lowercase_sentence = no_punct.lower()\n",
    "\n",
    "    # stem words\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    sentence = \" \".join([stemmer.stem(word) for word in lowercase_sentence.split()])\n",
    "    return sentence\n",
    "\n",
    "output_val_2_summary_id_caption = load_json_file(\"/home/grads/h/hasnat.md.abdullah/open_ended_activity_analysis/zero-shot-video-to-text/experiments/summary_study_activityNet/val_2 exp/output_val_2_summary_id_caption.json\")\n",
    "reference_captions = []\n",
    "candidate_captions = []\n",
    "print(len(output_val_2_summary_id_caption))\n",
    "for k,v in output_val_2_summary_id_caption.items():\n",
    "    if v['generated_summary']!=\"_\":\n",
    "        reference_captions = [remove_punctuation(caption).split() for caption in reference_captions]\n",
    "        candidate_captions = [remove_punctuation(caption).split() for caption in candidate_captions]\n",
    "        reference_captions.append(reference_captions)\n",
    "        candidate_captions.append(candidate_captions)\n",
    "\n",
    "\n",
    "\n",
    "# reference_captions[0]\n",
    "print(type(reference_captions))\n",
    "cal_cider_score(reference_captions,candidate_captions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oeaa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
